{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmCHf7bnfuyy"
      },
      "source": [
        "# HW3\n",
        "\n",
        "*deadline*: Update 26.04.2021 07:00"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7NmjinvfxF0"
      },
      "source": [
        "Using [transformers](https://github.com/huggingface/transformers) library solve the task [TERRa](https://russiansuperglue.com/tasks/task_info/TERRa). Textual Entailment Recognition for Russian is aimed to capture textual entailment in a binary classification form. Given two text fragments (premise and hypothesis), the task is to determine whether the meaning of the hypothesis is entailed from the premise.\n",
        "\n",
        "In this homework you should explore TERRa task; make a simple classifier with BERT embeddings to predict if the hypothesis is entailed from the premise text or not; finetune BERT-based model on TERRa task; make LM model and again finetune on the classification task; compare the results of classifiers and describe your results.\n",
        "\n",
        "You can also make submission to leaderboard https://www.kaggle.com/c/mipt-nlp-hw3-2021/.\n",
        "\n",
        "Final score will be computed by join between the tasks below and competition results https://www.kaggle.com/c/mipt-nlp-hw3-2021/.\n",
        "For the five top places you obtain additional points:\n",
        "\n",
        "* 1 - 10\n",
        "* 2 - 8\n",
        "* 3 - 6\n",
        "* 4 - 5\n",
        "* 5 - 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_sdVXXOguAf"
      },
      "source": [
        "### Rules\n",
        "\n",
        "* Homework is submitted through anytask\n",
        "* Homework should be done in a group of up to 3 people. \n",
        "If you are doing a task in a group, please indicate in anytask system the logins of the group members so that they can be found.\n",
        "* Homework is made in the form of a report either in a .pdf file, or in an ipython notebook.\n",
        "* The report should contain: \n",
        "  - the numbering of tasks and items that you completed, \n",
        "  - the solution code, and \n",
        "  - a clear step-by-step description of what you did. The report should be written in an academic style, without excessive use of slang and in compliance with the norms of the Russian language.\n",
        "* Do not copy fragments of lectures, articles and Wikipedia into your report.\n",
        "* Reports consisting solely of code will not be validated and will automatically be scored at zero.\n",
        "* Plagiarism and any unfair quotation leads to zeroing of the score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooPxcyV7P8C5"
      },
      "source": [
        "#### Part 0. [1 point] Get embeddings\n",
        "\n",
        "Take any BERT model (for example any from [DeepPavlov Ruberts](https://huggingface.co/DeepPavlov/rubert-base-cased)) and get BERT embeddings from the train dataset (or train + dev)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPZI5h1rguDx"
      },
      "source": [
        "#### Part 1. [2 point] Explore your data\n",
        "\n",
        "* Make a 2D reduction and draw plot for train labels.\n",
        "* Analyze your results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-lcOJlQguGZ"
      },
      "source": [
        "#### Part 2. [3 points] Make first classifier\n",
        "\n",
        "* Use pretrained embeddings as features to classifier. You can use for example SVM or LinearRegression.\n",
        "* Make crossvalidation and describe your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wPYE5x6g1dd"
      },
      "source": [
        "#### Part 3. [4 points] Finetune on Classification task\n",
        "\n",
        "* Finetune on a classification task with BERT-based model you chose in the 0 part (Example of code see [here](https://github.com/huggingface/transformers/tree/master/examples/text-classification))\n",
        "* Submit your results and get scores for your models. https://www.kaggle.com/c/mipt-nlp-hw3-2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skk90DcjUzyK"
      },
      "source": [
        "#### Part 4. [4 points] Finetune LM\n",
        "* Finetune language model with BERT-based model you chose in the 0 part (Example of code see [here](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py)).\n",
        "* Draw a plot (like you did in part 0 and 1)\n",
        "* Submit your results and get scores for your models. https://www.kaggle.com/c/mipt-nlp-hw3-2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEbrqDdSVcyH"
      },
      "source": [
        "#### Part 5. [4 points] Finetune on classification task your new LM model\n",
        "\n",
        "* Finetune on a classification task with BERT-based model you've done in part 4 Example of code see [here](https://github.com/huggingface/transformers/tree/master/examples/text-classification)\n",
        "* Submit your results and get scores for your models. https://www.kaggle.com/c/mipt-nlp-hw3-2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBtNUWBTuh2a"
      },
      "source": [
        "#### Part 6. [2 point] Results\n",
        "\n",
        "* Describe your results and experiments. Compare results from tree classifiers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geHqcANQYGaP"
      },
      "source": [
        "**TOTALLY: you can get maximum 20 points**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3ERCQmUWl4h"
      },
      "source": [
        "#### EXTRA\n",
        "\n",
        "After you have done the general part you can submit on the kaggle leaderboard any models for your solutions of TERRa task. Describe here your experiments.\n",
        "\n",
        "However! For submission the usage nof english models + translation of russian dataset is forbidden. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrSgHhczg4DE"
      },
      "source": [
        "#### Data\n",
        "\n",
        "Textual Entailment Recognition has been proposed recently as a generic task that captures major semantic inference needs across many NLP applications, such as Question Answering, Information Retrieval, Information Extraction, and Text Summarization. This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text.\n",
        "\n",
        "TERRa ia a part of Russian SuperGLUE benchmark, created like original English RTE dataset. You can download dataset [here](https://russiansuperglue.com/tasks/task_info/TERRa).\n",
        "For the whole homework use `train.jsonl` and `dev.jsonl` files. If you want to submit test to Russian SuperGLUE leaderboard - predict labels for the `test.jsonl` (check carefully submission format for this task on the Russian SuperGLUE website).\n",
        "\n",
        "Format of one example is the following:\n",
        "* premise - some text, background of the situation.\n",
        "* hypothesis - some hypothesis about situation based on premise.\n",
        "\n",
        "Task - to recognize when one piece of text entails another. \n",
        "\n",
        "Binary classification (two classes to predict): `entailment` or `not_entailment` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Os4qLtxfalq",
        "outputId": "0383f0c3-74cf-4bb0-81d8-d92336612ae1"
      },
      "source": [
        "!pip install jsonlines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/58/06f430ff7607a2929f80f07bfd820acbc508a4e977542fefcc522cde9dff/jsonlines-2.0.0-py3-none-any.whl\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7YWSMew3msD",
        "outputId": "2afefda2-a451-49e0-a82b-71b55afe9be3"
      },
      "source": [
        "import jsonlines\n",
        "import pprint\n",
        "\n",
        "with jsonlines.open('val.jsonl') as reader:\n",
        "    data = [row for row in reader]\n",
        "\n",
        "pprint.pprint(data[7])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hypothesis': 'Власти Грузии полностью выполняют подписанные соглашения о '\n",
            "               'взаимопонимании.',\n",
            " 'idx': 7,\n",
            " 'label': 'entailment',\n",
            " 'premise': 'Кроме того, власти Грузии подписали на добровольной основе '\n",
            "            'соглашения  о взаимопонимании, которые тоже полностью выполняют. '\n",
            "            'Что касается отношений непосредственно с представителями '\n",
            "            'грузинской власти, вы знаете, что наша миссия является миссией '\n",
            "            'нейтральной и независимой от грузинских властей, но мы пытаемся, '\n",
            "            'стремимся к тому, чтобы иметь наилучшие отношения со всеми '\n",
            "            'участниками Женевских переговоров. Но, конечно, мы придаем '\n",
            "            'большее внимание отношениям с грузинскими властями, так как были '\n",
            "            'приглашены грузинским государством, и мы находимся на грузинской '\n",
            "            'территории.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn1CchrLYtve"
      },
      "source": [
        "#### Baseline\n",
        "\n",
        "As a baseline for TERRa task we provide [Tf-idf solution](https://russiansuperglue.com/login/submit_info/183). We used a 20 thousand sample from Wikipedia, from Russian and English sites equally. We restricted a vocabulary to 10 thousand most common words. Then for a logistic regression was trained to predict an answer.\n",
        "\n",
        "An example of the training and usage you can find [here](https://github.com/RussianNLP/RussianSuperGLUE/blob/master/TFIDF%20baseline.ipynb) and [here](https://github.com/RussianNLP/RussianSuperGLUE/blob/master/tfidf_baseline/TERRa.py).\n"
      ]
    }
  ]
}
